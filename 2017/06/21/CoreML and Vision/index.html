<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> CoreML and Vision 入门学习 · iAlexSun's</title><meta name="description" content="CoreML and Vision 入门学习 - iAlexSun"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://yoursite.com/atom.xml" title="iAlexSun's"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="http://ac-pqj923nc.clouddn.com/0aac0dce4f2eff61dbe0.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/iAlexSun" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">CoreML and Vision 入门学习</h1><div class="post-info">Jun 21, 2017</div><div class="post-content"><p>今年的WWDC大会应该是近几年中有比较大动作的一次开发者大会，因为在最新的操作系统iOS11 SDK中增加了几个十分有重量的框架。主要是针对目前前沿科技研发的机器学习的库CoreML,Vision,ARKit。之前Apple一直在研究ML(Machine Learning)方面的的研究，只是在这次开发者大会上大大降低了开发者接入的门槛，这次算是为WWDC做一次笔记吧。<br><a id="more"></a></p>
<h2 id="1-CoreML介绍"><a href="#1-CoreML介绍" class="headerlink" title="1.CoreML介绍"></a>1.CoreML介绍</h2><p>自己动手写的一个WWDC中演示的实时对物体检测的Demo:<a href="https://github.com/iAlexSun/MySampleCode/tree/master/CMCoreMLDetect" target="_blank" rel="external">CMCoreMLDetect</a></p>
<p><img src="http://ac-mcm5vzvc.clouddn.com/5037428b2a248e15243c.gif" alt=""></p>
<p>CoreML(Core Machine Learning)是Apple在2017年WWDC推出十分重要的框架之一。CoreML接入是十分容易的，下面介绍一下如何接入我们的APP。我们可以<a href="https://developer.apple.com/machine-learning/" target="_blank" rel="external">下载</a>一个训练好的模型然后拖入到我们的APP中即可使用。<br><img src="http://ac-mcm5vzvc.clouddn.com/462eff3922d1f63480f4.png" alt=""><br><img src="http://ac-mcm5vzvc.clouddn.com/fc1f43b122fd4d9c3980.png" alt=""><br>Xcode会帮助我们自动生成对应的代码。文件名称就是模型的名字。使用的时候我们可以通过如下代码进行调用模型中的方法就可以对静态图片进行识别操作了，但是很多时候对静态图片的识别不能满足我们场景的需求。需要WWDC中介绍的对物体进行实时的物体扫描操作，检测物体。</p>
<p>对模型的加载可进行图片的处理和识别:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">guard let model = try? VNCoreMLModel(for: GoogLeNetPlaces().model) else &#123;</span><br><span class="line">      fatalError(&quot;Couldn&apos;t init model&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">    let request = VNCoreMLRequest(model: model, completionHandler: ResultsMethod)</span><br><span class="line">    let pixelBuffer = image.convertToPixelBuffer(image: image, imageFrame: 299.0)</span><br><span class="line">    let handler = VNImageRequestHandler(cvPixelBuffer:pixelBuffer!, options: [:])</span><br><span class="line">       </span><br><span class="line"> guard ((try? handler.perform([request])) != nil) else &#123;</span><br><span class="line">           fatalError(&quot;Error on model&quot;)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>如下代码是针对CoreML需要的图片格式进行处理封装的一个方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">func convertToPixelBuffer(image:UIImage,imageFrame:CGFloat) -&gt; CVPixelBuffer? &#123;</span><br><span class="line">        var pixelBuffer : CVPixelBuffer?</span><br><span class="line">        let imageDimension : CGFloat = imageFrame</span><br><span class="line">        let rgbColorSpace = CGColorSpaceCreateDeviceRGB()</span><br><span class="line">        let imageSize = CGSize(width:imageDimension, height:imageDimension)</span><br><span class="line">        let imageRect = CGRect(origin: CGPoint(x:0, y:0), size: imageSize)</span><br><span class="line">        </span><br><span class="line">        let options = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,</span><br><span class="line">                       kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary</span><br><span class="line">        </span><br><span class="line">        UIGraphicsBeginImageContextWithOptions(imageSize, true, 1.0)</span><br><span class="line">        image.draw(in:imageRect)</span><br><span class="line">        guard let newImage = UIGraphicsGetImageFromCurrentImageContext() else &#123;</span><br><span class="line">            return nil</span><br><span class="line">        &#125;</span><br><span class="line">        UIGraphicsEndImageContext()</span><br><span class="line">        let status = CVPixelBufferCreate(kCFAllocatorDefault,</span><br><span class="line">                                         Int(newImage.size.width),</span><br><span class="line">                                         Int(newImage.size.height),</span><br><span class="line">                                         kCVPixelFormatType_32ARGB,</span><br><span class="line">                                         options,</span><br><span class="line">                                         &amp;pixelBuffer)</span><br><span class="line">        guard (status == kCVReturnSuccess),</span><br><span class="line">            let uwPixelBuffer = pixelBuffer else &#123;</span><br><span class="line">                return nil</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        CVPixelBufferLockBaseAddress(uwPixelBuffer,</span><br><span class="line">                                     CVPixelBufferLockFlags(rawValue: 0))</span><br><span class="line">        let pixelData = CVPixelBufferGetBaseAddress(uwPixelBuffer)</span><br><span class="line">        let context = CGContext(data: pixelData,</span><br><span class="line">                                width: Int(newImage.size.width),</span><br><span class="line">                                height: Int(newImage.size.height),</span><br><span class="line">                                bitsPerComponent: 8,</span><br><span class="line">                                bytesPerRow: CVPixelBufferGetBytesPerRow(uwPixelBuffer),</span><br><span class="line">                                space: rgbColorSpace,</span><br><span class="line">                                bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue)</span><br><span class="line">        guard let uwContext = context else &#123;</span><br><span class="line">            return nil</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        uwContext.translateBy(x: 0, y: newImage.size.height)</span><br><span class="line">        uwContext.scaleBy(x: 1.0, y: -1.0)</span><br><span class="line">        </span><br><span class="line">        UIGraphicsPushContext(uwContext)</span><br><span class="line">        newImage.draw(in: CGRect(x: 0,</span><br><span class="line">                                 y: 0,</span><br><span class="line">                                 width: newImage.size.width,</span><br><span class="line">                                 height: newImage.size.height))</span><br><span class="line">        UIGraphicsPopContext()</span><br><span class="line">        CVPixelBufferUnlockBaseAddress(uwPixelBuffer, CVPixelBufferLockFlags(rawValue: 0))</span><br><span class="line">        return pixelBuffer</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://ac-mcm5vzvc.clouddn.com/3afddca5695270716da6.png" alt=""><br>其实目前来说对静态图片的检测就是这么简单的就可以实现了。</p>
<ul>
<li>第一步:将需要的<a href="https://developer.apple.com/machine-learning/" target="_blank" rel="external">模型下载</a>后拖入程序中。<br><img src="http://ac-mcm5vzvc.clouddn.com/56cb012d2998750cc73a.png" alt=""></li>
<li>第二步:对所需要检测的图片进行格式处理和转换。<br><img src="http://ac-mcm5vzvc.clouddn.com/432bd8a26f5429105463.png" alt=""></li>
<li>第三步:调用模型中的一些方法进行最后的图片检测,但是这只是其中最简单的需求部分，就如上文所说的可能我们使用的场景不会这样的简单，比如实时检测。<br><img src="http://ac-mcm5vzvc.clouddn.com/df3716756661407117f2.png" alt=""><h2 id="2-物体实时识别"><a href="#2-物体实时识别" class="headerlink" title="2.物体实时识别"></a>2.物体实时识别</h2>实时监测主要是开启摄像头然后获取每一帧图像然后进行相对应的格式进行转换然后再次进行图片的处理和识别基本的情况是这样。但是如果处理不得当会产生UI卡顿的的情况CPU飞起的情况。那么开启摄像头的主要代码如下:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">func openCamera()&#123;</span><br><span class="line">       captureSession.sessionPreset = AVCaptureSession.Preset.high</span><br><span class="line">       let devices = AVCaptureDevice.devices()</span><br><span class="line">       for device in devices &#123;</span><br><span class="line">           if (device.hasMediaType(AVMediaType.video)) &#123;</span><br><span class="line">               if (device.position == .back) &#123;</span><br><span class="line">                   cameraDevice = device</span><br><span class="line">                   if cameraDevice != nil &#123;</span><br><span class="line">                       beginSession()</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">func beginSession()&#123;</span><br><span class="line">       </span><br><span class="line">       let err : NSError? = nil</span><br><span class="line">       let captureDeviceInput = try? AVCaptureDeviceInput.init(device: cameraDevice)</span><br><span class="line">       captureSession.addInput(captureDeviceInput!)</span><br><span class="line">       let output = AVCaptureVideoDataOutput()</span><br><span class="line">       let cameraQueue = DispatchQueue(label:&quot;com.CoreMLDetect.cameraQueue&quot;)</span><br><span class="line">       output.setSampleBufferDelegate(self, queue:cameraQueue)</span><br><span class="line">       output.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]</span><br><span class="line">       captureSession.addOutput(output)</span><br><span class="line">       if err != nil &#123;</span><br><span class="line">           print(&quot;error: \(String(describing: err?.localizedDescription))&quot;)</span><br><span class="line">       &#125;</span><br><span class="line">       previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)</span><br><span class="line">       previewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill</span><br><span class="line">       previewLayer.frame = self.view.bounds</span><br><span class="line">       self.view.layer.addSublayer(previewLayer)</span><br><span class="line">       captureSession.startRunning()</span><br><span class="line">       </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>那么我们需要就是接收图像，在开启摄像头之后会有一个代理方法进行接收每一帧图像，在这个代理方法里面我们应该使用多线程处理图片防止CPU飞起主要代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)</span><br><span class="line">       let cameraImage = CIImage(cvPixelBuffer: pixelBuffer!)</span><br><span class="line">       let image = UIImage(ciImage: cameraImage)</span><br><span class="line">       let result = MLDetectImageModel(image: image)</span><br><span class="line">       VNDetectImageModel(image:image)</span><br><span class="line">       DispatchQueue.global().async &#123;</span><br><span class="line">           DispatchQueue.main.async &#123;</span><br><span class="line">               self.resultLabel.text = String.init(format:&quot;Inceptionv3:%@ %.2f&quot;, result.classLabel,result.classLabelProb)</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>
<p>基本上CoreML可以实现的入门Demo基本就告一段落了，目前来说CoreML没有被完全开发出来，iOS开发者能做到的也就是基本可以将训练好的模型带入到项目中进行后期的训练好的数据进行识别。在手机和其他设备上基本不会出现在线训练的情况的，目前对于机器学习训练模型是十分依赖硬件的，所以我们还是希望等待Apple后期如何出现更好的玩转机器学习这盘棋。</p>
<hr>
<h2 id="感想和猜想"><a href="#感想和猜想" class="headerlink" title="感想和猜想"></a>感想和猜想</h2><p>目前移动端开发工程师基本能做的很少，虽然目前Apple将门槛降低了很多，但是我们也只能做到很微弱的一部分，但是未来绘出现什么样的其他情况我们也不得而知，但是很希望CoreML会强大做到很优秀的程度。下面这个图片是我现在的想法。<br><img src="http://ac-mcm5vzvc.clouddn.com/149e0a56fec6416bf4a0.png" alt=""><br>未来如果可以动态加载模型进行动态的调用模型方法就可以大大减少项目的大小，目前来说一个普通级别的商业项目如果包含一个大一点的模型会多出500M左右，这个用户可能就不回很接受。所以未来可能有两个方向是需要改进的。第一个可能是可以在手机上动态训练模型，边训练边增强模型的准确度。第二个可能是动态将训练好的模型下载到APP中，其中有一个功能可以开启和模型相关的如果开启再下载就可以了，这样可以减少用户下载应用的烦恼。以上纯属作者自己YY。PS:图片部分来自<a href="https://devstreaming-cdn.apple.com/videos/wwdc/2017/710vxa4hl8hyb72/710/710_core_ml_in_depth.pdf" target="_blank" rel="external">WWDC</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/04/06/人脸检测/" class="next">下一篇</a></div><div class="copyright"><p>© 2015 - 2017 <a href="http://yoursite.com">iAlexSun</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script></body></html>